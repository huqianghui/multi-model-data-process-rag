{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal RAG Tutorial using GPT-4 and LlamaIndex\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction to Multimodal RAG\n",
    "2. Setup and Installation\n",
    "3. Preparing the Environment\n",
    "4. Data Collection and Preparation\n",
    "5. Building the Multimodal Index\n",
    "6. Implementing Multimodal Retrieval\n",
    "8. Multimodal RAG Querying\n",
    "\n",
    "\n",
    "## 1. Introduction to Multimodal RAG\n",
    "\n",
    "Multimodal Retrieval-Augmented Generation (RAG) is an advanced technique that combines text and image data to enhance the capabilities of large language models (LLMs) like GPT-4. This tutorial will guide you through the process of implementing a multimodal RAG system using GPT-4 and LlamaIndex.\n",
    "\n",
    "### How Multimodal RAG Works\n",
    "\n",
    "Multimodal RAG extends traditional text-based RAG by incorporating image data:\n",
    "\n",
    "1. **Indexing**: Both text and images are processed and stored in separate vector stores.\n",
    "2. **Retrieval**: When a query is received, relevant text and images are retrieved based on similarity.\n",
    "3. **Augmentation**: The retrieved information is used to augment the input to the LLM.\n",
    "4. **Generation**: The LLM generates a response based on the augmented input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a virtual environment\n",
    "# python -m venv multimodal_rag_env\n",
    "\n",
    "# Activate the virtual environment\n",
    "# On Windows:\n",
    "# multimodal_rag_env\\Scripts\\activate\n",
    "# On macOS and Linux:\n",
    "# source multimodal_rag_env/bin/activate\n",
    "\n",
    "# Install required packages\n",
    "%pip install llama-index-multi-modal-llms-openai\n",
    "%pip install llama-index-vector-stores-qdrant\n",
    "%pip install llama_index ftfy regex tqdm\n",
    "%pip install torch torchvision\n",
    "%pip install matplotlib scikit-image\n",
    "%pip install -U qdrant_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Set your OpenAI API key\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY') #\"your_api_key_here\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "input_image_path = Path(\"input_images\")\n",
    "data_path = Path(\"mixed_wiki\")\n",
    "\n",
    "for path in [input_image_path, data_path]:\n",
    "    if not path.exists():\n",
    "        Path.mkdir(path)\n",
    "\n",
    "print(\"Environment prepared successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%wget \"https://docs.google.com/uc?export=download&id=1nUhsBRiSWxcVQv8t8Cvvro8HJZ88LCzj\" -O ./input_images/long_range_spec.png\n",
    "%wget \"https://docs.google.com/uc?export=download&id=19pLwx0nVqsop7lo0ubUSYTzQfMtKJJtJ\" -O ./input_images/model_y.png\n",
    "%wget \"https://docs.google.com/uc?export=download&id=1utu3iD9XEgR5Sb7PrbtMf1qw8T1WdNmF\" -O ./input_images/performance_spec.png\n",
    "%wget \"https://docs.google.com/uc?export=download&id=1dpUakWMqaXR4Jjn1kHuZfB0pAXvjn2-i\" -O ./input_images/price.png\n",
    "%wget \"https://docs.google.com/uc?export=download&id=1qNeT201QAesnAP5va1ty0Ky5Q_jKkguV\" -O ./input_images/real_wheel_spec.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "image_paths = []\n",
    "for img_path in os.listdir(\"./input_images\"):\n",
    "    image_paths.append(str(os.path.join(\"./input_images\", img_path)))\n",
    "\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(2, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break\n",
    "\n",
    "\n",
    "plot_images(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genereate text descriptions for images using GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# put your local directore here\n",
    "image_documents = SimpleDirectoryReader(\"./input_images\").load_data()\n",
    "\n",
    "openai_mm_llm = OpenAIMultiModal(\n",
    "    model=\"gpt-4o\", api_key=OPENAI_API_KEY, max_new_tokens=1500\n",
    ")\n",
    "\n",
    "response_1 = openai_mm_llm.complete(\n",
    "    prompt=\"Generate detailed text description for each image.\",\n",
    "    image_documents=image_documents,\n",
    ")\n",
    "\n",
    "print(response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Collection and Preparation\n",
    "In this section, we'll collect text and image data from Wikipedia and other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_images(title):\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"imageinfo\",\n",
    "            \"iiprop\": \"url|dimensions|mime\",\n",
    "            \"generator\": \"images\",\n",
    "            \"gimlimit\": \"50\",\n",
    "        },\n",
    "    ).json()\n",
    "    image_urls = []\n",
    "    for page in response[\"query\"][\"pages\"].values():\n",
    "        if page[\"imageinfo\"][0][\"url\"].endswith((\".jpg\", \".png\")):\n",
    "            image_urls.append(page[\"imageinfo\"][0][\"url\"])\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Wikipedia titles to fetch\n",
    "wiki_titles = {\n",
    "    \"Tesla Model Y\",\n",
    "    \"Tesla Model X\",\n",
    "    \"Tesla Model 3\",\n",
    "    \"Tesla Model S\",\n",
    "    \"Kia EV6\",\n",
    "    \"BMW i3\",\n",
    "    \"Audi e-tron\",\n",
    "    \"Ford Mustang\",\n",
    "    \"Porsche Taycan\",\n",
    "    \"Rivian\",\n",
    "    \"Polestar\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch text and images\n",
    "import urllib\n",
    "\n",
    "image_uuid = 0\n",
    "MAX_IMAGES_PER_WIKI = 20\n",
    "\n",
    "for title in wiki_titles:\n",
    "    # Fetch text\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"explaintext\": True,\n",
    "        },\n",
    "    ).json()\n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "    wiki_text = page[\"extract\"]\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", \"w\") as fp:\n",
    "        fp.write(wiki_text)\n",
    "\n",
    "    # Fetch images\n",
    "    images_per_wiki = 0\n",
    "    list_img_urls = get_wikipedia_images(title)\n",
    "\n",
    "    for url in list_img_urls:\n",
    "        if url.endswith((\".jpg\", \".png\", \".svg\")):\n",
    "            image_uuid += 1\n",
    "            urllib.request.urlretrieve(\n",
    "                    url, data_path / f\"{image_uuid}.jpg\"\n",
    "                )\n",
    "            images_per_wiki += 1\n",
    "            if images_per_wiki >= MAX_IMAGES_PER_WIKI:\n",
    "                break\n",
    "\n",
    "print(\"Data collection completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%wget \"https://www.dropbox.com/scl/fi/mlaymdy1ni1ovyeykhhuk/tesla_2021_10k.htm?rlkey=qf9k4zn0ejrbm716j0gg7r802&dl=1\" -O ./mixed_wiki/tesla_2021_10k.htm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Multimodal Index\n",
    "Now that we have our data, let's build the multimodal index using LlamaIndex and Qdrant as our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "import qdrant_client\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "client = qdrant_client.QdrantClient(path=\"qdrant_mm_db\")\n",
    "\n",
    "text_store = QdrantVectorStore(client=client, collection_name=\"text_collection\")\n",
    "image_store = QdrantVectorStore(client=client, collection_name=\"image_collection\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# Create the MultiModal index\n",
    "documents = SimpleDirectoryReader(\"./mixed_wiki/\").load_data()\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "print(\"Multimodal index built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementing Multimodal Retrieval\n",
    "Let's create a retriever engine that can fetch both text and images based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode\n",
    "\n",
    "MAX_TOKENS = 50\n",
    "retriever_engine = index.as_retriever(\n",
    "    similarity_top_k=3, image_similarity_top_k=3\n",
    ")\n",
    "\n",
    "def retrieve_and_display(query):\n",
    "    retrieval_results = retriever_engine.retrieve(query[:MAX_TOKENS])\n",
    "\n",
    "    retrieved_images = []\n",
    "    for res_node in retrieval_results:\n",
    "        if isinstance(res_node.node, ImageNode):\n",
    "            retrieved_images.append(res_node.node.metadata[\"file_path\"])\n",
    "        else:\n",
    "            display_source_node(res_node, source_length=200)\n",
    "\n",
    "    if retrieved_images:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, img_path in enumerate(retrieved_images):\n",
    "            plt.subplot(1, len(retrieved_images), i+1)\n",
    "            img = Image.open(img_path)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_and_display(\"What is the best electric Sedan?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multimodal RAG Querying\n",
    "Finally, let's implement a multimodal RAG query engine that can answer questions using both text and image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.query_engine import SimpleMultiModalQueryEngine\n",
    "\n",
    "qa_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_tmpl = PromptTemplate(qa_tmpl_str)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=openai_mm_llm, text_qa_template=qa_tmpl\n",
    ")\n",
    "\n",
    "def multimodal_rag_query(query_str):\n",
    "    response = query_engine.query(query_str)\n",
    "    print(\"Answer:\", str(response))\n",
    "\n",
    "    print(\"\\nSources:\")\n",
    "    for text_node in response.metadata[\"text_nodes\"]:\n",
    "        display_source_node(text_node, source_length=200)\n",
    "\n",
    "    if response.metadata[\"image_nodes\"]:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, img_node in enumerate(response.metadata[\"image_nodes\"]):\n",
    "            plt.subplot(1, len(response.metadata[\"image_nodes\"]), i+1)\n",
    "            img = Image.open(img_node.metadata[\"file_path\"])\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "multimodal_rag_query(\"Compare the design features of Tesla Model S and Rivian R1\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
