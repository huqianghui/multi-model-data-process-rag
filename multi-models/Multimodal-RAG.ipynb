{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/multi_modal/multi_modal_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Multi-Modal Retrieval using GPT text embedding and CLIP image embedding for Wikipedia Articles\n",
    "\n",
    "In this notebook, we show how to build a Multi-Modal retrieval system using LlamaIndex.\n",
    "\n",
    "Wikipedia Text embedding index: Generate GPT text embeddings from OpenAI for texts\n",
    "\n",
    "Wikipedia Images embedding index: [CLIP](https://github.com/openai/CLIP) embeddings from OpenAI for images\n",
    "\n",
    "\n",
    "Query encoder:\n",
    "* Encoder query text for text index using GPT embedding\n",
    "* Encoder query text for image index using CLIP embedding\n",
    "\n",
    "Framework: [LlamaIndex](https://github.com/run-llama/llama_index)\n",
    "\n",
    "Steps:\n",
    "1. Download texts and images raw files for Wikipedia articles\n",
    "2. Build text index for vector store using GPT embeddings\n",
    "3. Build image index for vector store using CLIP embeddings\n",
    "4. Retrieve relevant text and image simultaneously using different query encoding embeddings and vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q llama-index-vector-stores-qdrant\n",
    "%pip install llama_index ftfy regex tqdm\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "%pip install torch torchvision\n",
    "%pip install matplotlib scikit-image\n",
    "%pip install -U qdrant_clien\n",
    "%pip install llama-index-embeddings-clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Download Multi-Modal datasets including texts and images from \n",
    "\n",
    "---\n",
    "\n",
    "Wikipedia\n",
    "Parse wikipedia articles and save into local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "wiki_titles = [\n",
    "    \"RoboCop\",\n",
    "    \"Labour Party (UK)\",\n",
    "    \"SpaceX\",\n",
    "    \"OpenAI\",\n",
    "]\n",
    "\n",
    "\n",
    "data_path = Path(\"data_wiki\")\n",
    "\n",
    "for title in wiki_titles:\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"explaintext\": True,\n",
    "        },\n",
    "    ).json()\n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "    wiki_text = page[\"extract\"]\n",
    "\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", \"w\") as fp:\n",
    "        fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Wikipedia Images and texts. Load into local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import urllib.request\n",
    "\n",
    "image_path = Path(\"data_wiki\")\n",
    "image_uuid = 0\n",
    "# image_metadata_dict stores images metadata including image uuid, filename and path\n",
    "image_metadata_dict = {}\n",
    "MAX_IMAGES_PER_WIKI = 30\n",
    "\n",
    "wiki_titles = [\n",
    "    \"RoboCop\",\n",
    "    \"Labour Party (UK)\",\n",
    "    \"SpaceX\",\n",
    "    \"OpenAI\",\n",
    "]\n",
    "\n",
    "# create folder for images only\n",
    "if not image_path.exists():\n",
    "    Path.mkdir(image_path)\n",
    "\n",
    "\n",
    "# Download images for wiki pages\n",
    "# Assing UUID for each image\n",
    "for title in wiki_titles:\n",
    "    images_per_wiki = 0\n",
    "    print(title)\n",
    "    try:\n",
    "        page_py = wikipedia.page(title)\n",
    "        list_img_urls = page_py.images\n",
    "        for url in list_img_urls:\n",
    "            if url.endswith(\".jpg\") or url.endswith(\".png\"):\n",
    "                image_uuid += 1\n",
    "                image_file_name = title + \"_\" + url.split(\"/\")[-1]\n",
    "\n",
    "                # img_path could be s3 path pointing to the raw image file in the future\n",
    "                image_metadata_dict[image_uuid] = {\n",
    "                    \"filename\": image_file_name,\n",
    "                    \"img_path\": \"./\" + str(image_path / f\"{image_uuid}.jpg\"),\n",
    "                }\n",
    "                urllib.request.urlretrieve(\n",
    "                    url, image_path / f\"{image_uuid}.jpg\"\n",
    "                )\n",
    "                images_per_wiki += 1\n",
    "                # Limit the number of images downloaded per wiki page to 15\n",
    "                if images_per_wiki > MAX_IMAGES_PER_WIKI:\n",
    "                    break\n",
    "    except:\n",
    "        print(str(Exception(\"No images found for Wikipedia page: \")) + title)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Multi Modal Vector Store using Text and Image embeddings under different collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "\n",
    "# Create a local Qdrant vector store\n",
    "client = qdrant_client.QdrantClient(path=\"qdrant_d_0\")\n",
    "\n",
    "text_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"text_collection_0\"\n",
    ")\n",
    "image_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"image_collection_0\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "# Create the MultiModal index\n",
    "documents = SimpleDirectoryReader(\"./data_wiki/\").load_data()\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot downloaded Images from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_images(image_metadata_dict):\n",
    "    original_images_urls = []\n",
    "    images_shown = 0\n",
    "    for image_id in image_metadata_dict:\n",
    "        img_path = image_metadata_dict[image_id][\"img_path\"]\n",
    "        if os.path.isfile(img_path):\n",
    "            filename = image_metadata_dict[image_id][\"filename\"]\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            plt.subplot(8, 8, len(original_images_urls) + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            original_images_urls.append(filename)\n",
    "            images_shown += 1\n",
    "            if images_shown >= 64:\n",
    "                break\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plot_images(image_metadata_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a separate CLIP image embedding index under a differnt collection `wikipedia_img`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        if os.path.isfile(img_path):\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(2, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Multi-Modal retrieval results for some example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"what is the Labour Party?\"\n",
    "# generate  retrieval results\n",
    "retriever = index.as_retriever(similarity_top_k=3, image_similarity_top_k=5)\n",
    "retrieval_results = retriever.retrieve(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode\n",
    "\n",
    "retrieved_image = []\n",
    "for res_node in retrieval_results:\n",
    "    if isinstance(res_node.node, ImageNode):\n",
    "        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "    else:\n",
    "        display_source_node(res_node, source_length=200)\n",
    "\n",
    "plot_images(retrieved_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"Who created RoboCop?\"\n",
    "# generate  retrieval results\n",
    "retriever = index.as_retriever(similarity_top_k=3, image_similarity_top_k=5)\n",
    "retrieval_results = retriever.retrieve(test_query)\n",
    "\n",
    "retrieved_image = []\n",
    "for res_node in retrieval_results:\n",
    "    if isinstance(res_node.node, ImageNode):\n",
    "        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "    else:\n",
    "        display_source_node(res_node, source_length=200)\n",
    "\n",
    "plot_images(retrieved_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"What does OpenAI do?\"\n",
    "# generate  retrieval results\n",
    "retriever = index.as_retriever(similarity_top_k=3, image_similarity_top_k=5)\n",
    "retrieval_results = retriever.retrieve(test_query)\n",
    "\n",
    "retrieved_image = []\n",
    "for res_node in retrieval_results:\n",
    "    if isinstance(res_node.node, ImageNode):\n",
    "        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "    else:\n",
    "        display_source_node(res_node, source_length=200)\n",
    "\n",
    "plot_images(retrieved_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"which company makes Tesla\"\n",
    "# generate  retrieval results\n",
    "retriever = index.as_retriever(similarity_top_k=3, image_similarity_top_k=5)\n",
    "retrieval_results = retriever.retrieve(test_query)\n",
    "\n",
    "retrieved_image = []\n",
    "for res_node in retrieval_results:\n",
    "    if isinstance(res_node.node, ImageNode):\n",
    "        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "    else:\n",
    "        display_source_node(res_node, source_length=200)\n",
    "\n",
    "plot_images(retrieved_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
