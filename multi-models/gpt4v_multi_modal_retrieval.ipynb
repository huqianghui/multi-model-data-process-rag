{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3bbc9a0e",
      "metadata": {
        "id": "3bbc9a0e"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/multi_modal/gpt4v_multi_modal_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Advanced Multi-Modal Retrieval using GPT4V and Multi-Modal Index/Retriever\n",
        "\n",
        "In this notebook, we show how to build a Multi-Modal retrieval system using LlamaIndex with GPT4-V and CLIP.\n",
        "\n",
        "LlamaIndex Multi-Modal Retrieval\n",
        "\n",
        "- Text embedding index: Generate GPT text embeddings\n",
        "- Images embedding index: [CLIP](https://github.com/openai/CLIP) embeddings from OpenAI for images\n",
        "\n",
        "\n",
        "Encoding queries:\n",
        "* Encode query text for text index using ada\n",
        "* Encode query text for image index using CLIP\n",
        "\n",
        "Framework: [LlamaIndex](https://github.com/run-llama/llama_index)\n",
        "\n",
        "Steps:\n",
        "1. Using Multi-Modal LLM GPT4V class to undertand multiple images\n",
        "2. Download texts, images, pdf raw files from related Wikipedia articles and SEC 10K report\n",
        "2. Build Multi-Modal index and vetor store for both texts and images\n",
        "4. Retrieve relevant text and image simultaneously using Multi-Modal Retriver according to the image reasoning from Step 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc691ca8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc691ca8",
        "outputId": "c8e64921-a90c-4e28-b414-79343dd78e29"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/run-llama/llama_index.git\n",
        "%pip install ftfy regex tqdm\n",
        "%pip install git+https://github.com/openai/CLIP.git\n",
        "%pip install torch torchvision\n",
        "%pip install matplotlib scikit-image\n",
        "%pip install -U qdrant_client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e648a24e",
      "metadata": {
        "id": "e648a24e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "OPENAI_API_TOKEN = \"sk-...\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7683e4ed",
      "metadata": {
        "id": "7683e4ed"
      },
      "source": [
        "## Download our set of images of cars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b383f38e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b383f38e",
        "outputId": "f3c23ba7-e3cb-4b50-ab6e-a0a87349bc8f"
      },
      "outputs": [],
      "source": [
        "!rm -rf ./cars\n",
        "!rm -rf ./__MACOSX\n",
        "!wget \"https://www.dropbox.com/scl/fi/48f5vfp86ak56tv6647ru/cars.zip?rlkey=8vre28paljf4ovf5269a1w0as&dl=0\" -O cars.zip -q\n",
        "!unzip cars.zip -d ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffb1327",
      "metadata": {
        "id": "0ffb1327"
      },
      "source": [
        "### Plot input images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5a64bb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "e5a64bb6",
        "outputId": "fad51f73-5df3-426c-b4a7-c411703f794e"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "image_paths = []\n",
        "for img_path in os.listdir(\"./cars\"):\n",
        "    image_paths.append(str(os.path.join(\"./cars\", img_path)))\n",
        "\n",
        "\n",
        "def plot_images(image_paths):\n",
        "    images_shown = 0\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    for img_path in image_paths:\n",
        "        if \".jpg\" in img_path:\n",
        "            image = Image.open(img_path)\n",
        "\n",
        "            plt.subplot(3, 3, images_shown + 1)\n",
        "            plt.imshow(image)\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "\n",
        "            images_shown += 1\n",
        "            if images_shown >= 9:\n",
        "                break\n",
        "\n",
        "\n",
        "plot_images(image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e54694f",
      "metadata": {
        "id": "4e54694f"
      },
      "source": [
        "### Using GPT4V to understand those input images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8e3f9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a8e3f9a",
        "outputId": "b2eaec5b-5d64-4e0c-9ab0-54c4f586d8af"
      },
      "outputs": [],
      "source": [
        "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "image_documents = SimpleDirectoryReader(input_files=[\"./cars/o1.jpg\",\"./cars/t1.jpg\",\"./cars/v1.jpg\"]).load_data()\n",
        "\n",
        "openai_mm_llm = OpenAIMultiModal(\n",
        "    model=\"gpt-4-vision-preview\", api_key=OPENAI_API_TOKEN, max_new_tokens=1500\n",
        ")\n",
        "\n",
        "response_1 = openai_mm_llm.complete(\n",
        "    prompt=\"Describe the car in the image\",\n",
        "    image_documents=image_documents,\n",
        ")\n",
        "\n",
        "print(response_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uk4XVjnLJgF2",
      "metadata": {
        "id": "uk4XVjnLJgF2"
      },
      "outputs": [],
      "source": [
        "!rm -rf qdrant_mm_db"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f476d8a",
      "metadata": {
        "id": "4f476d8a"
      },
      "source": [
        "## Build Multi-modal index and vector store to index both text and images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d13318",
      "metadata": {
        "id": "90d13318"
      },
      "outputs": [],
      "source": [
        "from llama_index.indices.multi_modal.base import MultiModalVectorStoreIndex\n",
        "from llama_index.vector_stores import QdrantVectorStore\n",
        "from llama_index import SimpleDirectoryReader, StorageContext\n",
        "\n",
        "import qdrant_client\n",
        "from llama_index import (\n",
        "    SimpleDirectoryReader,\n",
        ")\n",
        "\n",
        "# Create a local Qdrant vector store\n",
        "client = qdrant_client.QdrantClient(path=\"qdrant_mm_db\")\n",
        "\n",
        "text_store = QdrantVectorStore(\n",
        "    client=client, collection_name=\"text_collection\"\n",
        ")\n",
        "image_store = QdrantVectorStore(\n",
        "    client=client, collection_name=\"image_collection\"\n",
        ")\n",
        "storage_context = StorageContext.from_defaults(vector_store=text_store,image_store=image_store)\n",
        "\n",
        "# Create the MultiModal index\n",
        "documents = SimpleDirectoryReader(\"./cars\").load_data()\n",
        "index = MultiModalVectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b6f7ab8",
      "metadata": {
        "id": "9b6f7ab8"
      },
      "source": [
        "## Query text and images from our Multi-Modal Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f907277-c23a-4809-8e4f-52837c8e24ce",
      "metadata": {
        "id": "2f907277-c23a-4809-8e4f-52837c8e24ce"
      },
      "source": [
        "### Find the Toyotas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "815a140b",
      "metadata": {
        "id": "815a140b"
      },
      "outputs": [],
      "source": [
        "# generate Text retrieval results\n",
        "retriever_engine = index.as_retriever(\n",
        "    similarity_top_k=3, image_similarity_top_k=3\n",
        ")\n",
        "retrieval_results = retriever_engine.retrieve(\"Find the Toyotas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac80ce17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "ac80ce17",
        "outputId": "24edaf22-e663-4f6c-eb27-1149aaf380df"
      },
      "outputs": [],
      "source": [
        "from llama_index.response.notebook_utils import display_source_node\n",
        "from llama_index.schema import ImageNode\n",
        "\n",
        "retrieved_image = []\n",
        "for res_node in retrieval_results:\n",
        "    if isinstance(res_node.node, ImageNode):\n",
        "        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
        "    else:\n",
        "        display_source_node(res_node, source_length=200)\n",
        "\n",
        "plot_images(retrieved_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cXdYcfjMMhU_",
      "metadata": {
        "id": "cXdYcfjMMhU_"
      },
      "source": [
        "## Look for Teslas instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gVAuaUZ2Memr",
      "metadata": {
        "id": "gVAuaUZ2Memr"
      },
      "outputs": [],
      "source": [
        "# generate Text retrieval results\n",
        "retriever_engine = index.as_retriever(\n",
        "    similarity_top_k=3, image_similarity_top_k=3\n",
        ")\n",
        "retrieval_results = retriever_engine.retrieve(\"Find the Teslas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mh4kEohRMsRA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "Mh4kEohRMsRA",
        "outputId": "db33575b-7f4b-43f5-b7ce-e74e5f889062"
      },
      "outputs": [],
      "source": [
        "from llama_index.response.notebook_utils import display_source_node\n",
        "from llama_index.schema import ImageNode\n",
        "\n",
        "retrieved_image = []\n",
        "for res_node in retrieval_results:\n",
        "    if isinstance(res_node.node, ImageNode):\n",
        "        retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
        "    else:\n",
        "        display_source_node(res_node, source_length=200)\n",
        "\n",
        "plot_images(retrieved_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xbEKGa-vkd-t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbEKGa-vkd-t",
        "outputId": "b0b7d14a-9461-4983-aedf-93051cc58819"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=3, image_similarity_top_k=3\n",
        ")\n",
        "response = query_engine.query(\"Compare the toyotas\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
